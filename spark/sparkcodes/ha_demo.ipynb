{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop HA Demo - Spark Edition\n",
    "\n",
    "This notebook demonstrates Hadoop High Availability features during Spark job execution.\n",
    "\n",
    "**Demo Flow:**\n",
    "- üìä **Phase 1**: Sales data processing (trigger NameNode failover)\n",
    "- üë• **Phase 2**: User events analysis (trigger ResourceManager failover)\n",
    "- üìà **Phase 3**: Summary report generation\n",
    "\n",
    "**Expected Duration:** ~5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Create Spark session for YARN cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hadoop-HA-Demo-Interactive\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark session created successfully\")\n",
    "print(f\"   Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Sales Data Processing\n",
    "\n",
    "**üéØ Perfect time to trigger NameNode failover via the dashboard!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Processing sales data...\")\n",
    "\n",
    "# Read sales data from HDFS\n",
    "sales_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/demo/data/sales_data.csv\")\n",
    "\n",
    "print(f\"   ‚úì Loaded {sales_df.count()} sales records\")\n",
    "sales_df.show(5)\n",
    "\n",
    "# Strategic delay for NameNode failover demo\n",
    "print(\"\\n‚è±Ô∏è  Waiting 30 seconds... (TRIGGER NAMENODE FAILOVER NOW!)\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Calculate daily sales by region\n",
    "daily_sales = sales_df.groupBy(\"date\", \"region\") \\\n",
    "    .agg(\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        round(sum(col(\"quantity\") * col(\"price\")), 2).alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"transaction_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"date\", \"region\")\n",
    "\n",
    "daily_sales.cache()\n",
    "print(\"   ‚úì Calculated daily sales aggregations\")\n",
    "daily_sales.show()\n",
    "\n",
    "# Write to HDFS\n",
    "daily_sales.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/demo/output/daily_sales_summary\")\n",
    "\n",
    "print(\"   ‚úÖ Phase 1 completed - Sales data processed and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: User Events Analysis\n",
    "\n",
    "**üéØ Perfect time to trigger ResourceManager failover via the dashboard!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üë• Processing user events...\")\n",
    "\n",
    "# Strategic delay for ResourceManager failover demo\n",
    "print(\"\\n‚è±Ô∏è  Waiting 30 seconds... (TRIGGER RESOURCEMANAGER FAILOVER NOW!)\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Read user events\n",
    "events_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/demo/data/user_events.csv\")\n",
    "\n",
    "print(f\"   ‚úì Loaded {events_df.count()} user events\")\n",
    "events_df.show(5)\n",
    "\n",
    "# Analyze user behavior by device type\n",
    "device_analysis = events_df.groupBy(\"device_type\", \"event_type\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        countDistinct(\"session_id\").alias(\"unique_sessions\")\n",
    "    ) \\\n",
    "    .orderBy(\"device_type\", \"event_type\")\n",
    "\n",
    "device_analysis.cache()\n",
    "print(\"   ‚úì Analyzed user behavior by device\")\n",
    "device_analysis.show()\n",
    "\n",
    "# Write device analysis\n",
    "device_analysis.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/demo/output/device_analysis\")\n",
    "\n",
    "print(\"   ‚úÖ Phase 2 completed - User events analyzed and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Summary Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Generating summary report...\")\n",
    "\n",
    "# Add final delay\n",
    "time.sleep(20)\n",
    "\n",
    "# Calculate total metrics\n",
    "total_sales = daily_sales.agg(\n",
    "    sum(\"total_quantity\").alias(\"grand_total_quantity\"),\n",
    "    sum(\"total_revenue\").alias(\"grand_total_revenue\"),\n",
    "    count(\"*\").alias(\"total_daily_records\")\n",
    ").collect()[0]\n",
    "\n",
    "total_events = device_analysis.agg(\n",
    "    sum(\"event_count\").alias(\"grand_total_events\"),\n",
    "    sum(\"unique_users\").alias(\"total_unique_users\")\n",
    ").collect()[0]\n",
    "\n",
    "# Create summary data\n",
    "summary_data = [\n",
    "    (\"total_sales_quantity\", str(total_sales[\"grand_total_quantity\"])),\n",
    "    (\"total_sales_revenue\", str(total_sales[\"grand_total_revenue\"])),\n",
    "    (\"total_user_events\", str(total_events[\"grand_total_events\"])),\n",
    "    (\"total_unique_users\", str(total_events[\"total_unique_users\"])),\n",
    "    (\"processing_timestamp\", str(int(time.time())))\n",
    "]\n",
    "\n",
    "summary_schema = StructType([\n",
    "    StructField(\"metric\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "summary_df = spark.createDataFrame(summary_data, summary_schema)\n",
    "\n",
    "# Write summary report\n",
    "summary_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/demo/output/summary_report\")\n",
    "\n",
    "print(\"   ‚úì Generated and saved summary report\")\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Results\n",
    "\n",
    "**üèÜ DEMO COMPLETED SUCCESSFULLY!**\n",
    "\n",
    "Your Hadoop HA cluster has successfully survived:\n",
    "- ‚úÖ NameNode failover during data processing\n",
    "- ‚úÖ ResourceManager failover during job execution\n",
    "- ‚úÖ All data operations completed without interruption\n",
    "- ‚úÖ Results are accessible from both active/standby states\n",
    "\n",
    "**üí™ Hadoop HA: Zero downtime, maximum reliability!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final results in HDFS\n",
    "print(\"üìÇ Final output files in HDFS:\")\n",
    "print(\"   ‚Ä¢ /demo/output/daily_sales_summary/\")\n",
    "print(\"   ‚Ä¢ /demo/output/device_analysis/\")\n",
    "print(\"   ‚Ä¢ /demo/output/summary_report/\")\n",
    "print(\"\\nüéâ All files successfully written despite failovers!\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"\\nüõë Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
